{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62a85db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17695d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_train = fetch_20newsgroups(subset ='train')\n",
    "news_test = fetch_20newsgroups(subset ='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ed4e047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n"
     ]
    }
   ],
   "source": [
    "print(news_train.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a219f785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "print(news_train.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2621172b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(news_train.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cefa0d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4919532a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the NLTK stopwords and stemmer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a66fa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to clean the text data\n",
    "def clean(text):\n",
    "    \n",
    "    # Removing email headers\n",
    "    text = text.split('\\n\\n', 1)[-1]\n",
    "    \n",
    "    # Removing any leading or trailing white space\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Removing any quoted text\n",
    "    text = '\\n'.join([line for line in text.split('\\n') if not line.startswith('>')])\n",
    "    \n",
    "    # Removing any URLs\n",
    "    text = ' '.join([word for word in text.split() if not word.startswith('http')])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f2da363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to preprocess the text data\n",
    "def preprocess(text):\n",
    "    \n",
    "    # Removing punctuation marks and converting to lowercase\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation)).lower()\n",
    "    \n",
    "    # Tokenizing the text into words\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Removing stop words and stem the remaining words\n",
    "    words = [stemmer.stem(word) for word in words if word not in stop_words]\n",
    "    \n",
    "    # Joining the words back into a string\n",
    "    text = ' '.join(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb674b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning and preprocessing the text data in the dataset\n",
    "preprocessed_train_data = []\n",
    "for text in news_train.data:\n",
    "    cleaned_text = clean(text)\n",
    "    preprocessed_text = preprocess(cleaned_text)\n",
    "    preprocessed_train_data.append(preprocessed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c5c6abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_test_data = []\n",
    "for text in news_test.data:\n",
    "    cleaned_text = clean(text)\n",
    "    preprocessed_text = preprocess(cleaned_text)\n",
    "    preprocessed_test_data.append(preprocessed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb5ea198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bb0ae50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_vectorizer(preprocessed_data):\n",
    "    # Counting the number of documents in which each term appears\n",
    "    document_frequency = {}\n",
    "    for document in preprocessed_train_data:\n",
    "        terms = set(document.split())\n",
    "        for term in terms:\n",
    "            if term in document_frequency:\n",
    "                document_frequency[term] += 1\n",
    "            else:\n",
    "                document_frequency[term] = 1\n",
    "\n",
    "    # Computing the IDF for each term\n",
    "    num_documents = len(preprocessed_train_data)\n",
    "    inverse_document_frequency = {}\n",
    "    for term in document_frequency:\n",
    "        inverse_document_frequency[term] = np.log(num_documents / document_frequency[term])\n",
    "\n",
    "    # Computing the TF-IDF vector for each document\n",
    "    tfidf_vectors = []\n",
    "    for document in preprocessed_train_data:\n",
    "        terms = document.split()\n",
    "        if len(terms) == 0:\n",
    "            continue\n",
    "        tfidf_vector = []\n",
    "        for term in inverse_document_frequency:\n",
    "            tf = terms.count(term) / len(terms)\n",
    "            idf = inverse_document_frequency[term]\n",
    "            tfidf_vector.append(tf * idf)\n",
    "        tfidf_vectors.append(tfidf_vector)\n",
    "    \n",
    "    return tfidf_vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c274c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectors_train = tfidf_vectorizer(preprocessed_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc01f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectors_test = tfidf_vectorizer(preprocessed_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a37c095",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultinomialNB:\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        num_docs, num_words = X.shape\n",
    "        self.classes = np.unique(y)\n",
    "        num_classes = len(self.classes)\n",
    "\n",
    "        # Calculate class priors\n",
    "        self.class_priors = np.zeros(num_classes, dtype=np.float64)\n",
    "        for i, c in enumerate(self.classes):\n",
    "            self.class_priors[i] = np.sum(y == c) / num_docs\n",
    "\n",
    "        # Calculate word frequencies and conditional probabilities\n",
    "        self.word_freqs = np.zeros((num_classes, num_words), dtype=np.int64)\n",
    "        for i, c in enumerate(self.classes):\n",
    "            X_c = X[y == c]\n",
    "            self.word_freqs[i, :] = np.sum(X_c, axis=0)\n",
    "        total_word_freqs = np.sum(self.word_freqs, axis=1, keepdims=True)\n",
    "        self.cond_probs = (self.word_freqs + self.alpha) / (total_word_freqs + self.alpha * num_words)\n",
    "\n",
    "    def predict(self, X):\n",
    "        log_probs = np.log(self.class_priors) + X @ np.log(self.cond_probs.T)\n",
    "        return self.classes[np.argmax(log_probs, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b6dd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(tfidf_vectors_train, news_train.target)\n",
    "predictions = nb_classifier.predict(tfidf_vectors_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e048e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(news_test.target, predictions)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256d270b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
